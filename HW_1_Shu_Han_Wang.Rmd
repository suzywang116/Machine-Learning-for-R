---
title: "HW 1"
author: "Shu-Han Wang"
output:
  pdf_document: default
  html_document:
    number_sections: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warnings = FALSE, fig.align = 'center',  eval = TRUE)
```

We will be predicting the housing price using the `sahp` dataset in the **r02pro** package. Please answer the following questions.

You can run the following code to prepare the analysis.
```{r}
library(r02pro)     #INSTALL IF NECESSARY
library(tidyverse)  #INSTALL IF NECESSARY
my_sahp <- sahp %>% 
  na.omit() %>%
  select(gar_car, liv_area, kit_qual, sale_price) %>%
  mutate_at(vars(kit_qual), as.factor)  #change the variable to factor
my_sahp_train <- my_sahp[1:100, ]
my_sahp_test <- my_sahp[-(1:100), ]
```

1. Using the training data `my_sahp_train` to fit a simple linear regression model of `sale_price` on each variable (`gar_car`, `liv_area`, `kit_qual`) separately. For each regression,

    a. Interpret the coefficients and compute the $R^2$. Which variable is most useful in predicting the `sale_price`?
    b. Comput the fitted value and the prediction on the test data, then compute the training and test error. Which variable gives the smallest test error? Does this agree with the variable with the highest $R^2$? Explain your findings.
Ans_a: "kit_qual" is most useful in predicting the "sale_price".
Ans_b: "liv_area" has the smallest test error. The result doesn't match the result with the highest R2, maybe it caused by the over-fitting.
```{r  Q1, warning = FALSE}
# simple regression, compute R2
variable <- c("gar_car", "liv_area", "kit_qual")
for (i in variable){
  func <- paste0("sale_price", "~", i)   
  lmod <- lm(func, my_sahp_train)
  print(summary(lmod))
  fitted_value <- lmod$fitted.values # the fitted value
  pred_test <- predict(lmod, my_sahp_test)  # prediction on the test data
  train_error <- sum((fitted_value - my_sahp_train$sale_price)^2)/nrow(my_sahp_train)
  test_error <- sum((pred_test - my_sahp_test$sale_price)^2)/nrow(my_sahp_test)
  df <- data.frame (R2=summary(lmod)$r.squared, train_error = train_error, test_error = test_error)
  print(df)}
```

2. Using the training data `my_sahp_train` to fit a linear regression model of `sale_price` on all variables, interpret the coefficients and compute the $R^2$. Then compute the training and test error. Compare the results to Q1 and explain your findings.
Ans: When we consider more variables, the R2 becomes bigger, and the training & test errors all become smaller. The result shows it's better to consider more variables for linear regression model.
```{r Q2, warning = FALSE}
# regression model on all variables
lmod_all <- lm(sale_price~., my_sahp_train)
summary(lmod_all)
train_error_all <- sum((lmod_all$fitted.values - my_sahp_train$sale_price)^2)/nrow(my_sahp_train)
test_error_all <- sum((predict(lmod_all, my_sahp_test) - my_sahp_test$sale_price)^2)/nrow(my_sahp_test)
df_all <- data.frame (R2=summary(lmod_all)$r.squared, train_error = train_error_all, test_error = test_error_all)
print(df_all)
```


3. Now, use the KNN method for predicting the `sale_price` using all predictors. 
    a. Vary the nearest number $K$ from 1 to 50 with increment 1. For each $K$, fit the KNN regression model on the training data, and predict on the test data. Visualize the training and test error trend as a function of $K$. Discuss your findings.
    b. Compare the best KNN result with the linear regression result in Q2. Discuss your findings. 
Ans_a: Whether the K is too small or too big won't make a good prediction to the dataset. The best choice of K is around 5~10, and when K = 9, we have the lowest test error. 
Ans_b: I think multiple linear regression has better result.
```{r Q3, warning = FALSE}
library(caret)   
# Split data to predictors & outcome
train_x <- subset(my_sahp_train, select = -sale_price)
train_y <- my_sahp_train$sale_price
test_x <- subset(my_sahp_test, select = -sale_price)
test_y <- my_sahp_test$sale_price

# turn categorical variable into numerical variables 
train_x$kit_qual <- as.numeric(train_x$kit_qual)
test_x$kit_qual <- as.numeric(test_x$kit_qual)


# KNN: Different choice of $K$
pred_dat <- NULL
k_seq <- c (1:50)
test_error_seq <- train_error_seq <- rep(0, 50)
for(k in seq_along(k_seq)){
  fit <- knnreg(train_x, train_y, k = k_seq[k])
  y_test_hat <- predict(fit, test_x)
  test_error_seq[k] <- sum((test_y - y_test_hat)^2)/length(test_y)
  y_train_hat <- predict(fit, train_x)
  train_error_seq[k] <- sum((train_y - y_train_hat)^2)/length(train_y)
  pred_dat <- rbind(pred_dat,
                    data.frame(x = test_x,
                               y = y_test_hat,
                               y_true = test_y,
                               k = k_seq[k]))
}
df <- data.frame(k = k_seq, train_error = train_error_seq, test_error = test_error_seq)
df
# Visualization
ggplot(df) +
  geom_point(mapping = aes(x = k, y = train_error), alpha = 0.5, color = "green") + 
  geom_point(mapping = aes(x = k, y = test_error), alpha = 0.5,  color = "red")
```

4. ISLR book 2nd Edition Chapter 3.7 Question 6

$$
\hat \beta_1 = \frac{\sum^n_{i=1}(x_i - \bar x)(y_i - \bar y)}{\sum^n_{i=1}(x_i - \bar x)^2}
$$
$$
\hat \beta_0 = \bar y - \hat \beta_1 \bar x
$$
where $\bar y = \frac{1}{n}\sum^n_{i=1}y_i$ and $\bar x = \frac{1}{n}\sum^n_{i=1}x_i$

Using (3.4 least squares coefficient estimates - see equations above), argue that in the case of simple linear regression, the least squares line always passes through the point $(\bar x, \bar y$).

$$
x_i = \bar x
$$
$$
\hat \beta_1 = 0
$$
$$
\hat \beta_0 = \bar y 
$$
